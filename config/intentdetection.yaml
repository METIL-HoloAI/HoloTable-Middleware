endpoint: "https://api.openai.com/v1/chat/completions"
method: "POST"
headers:
  Authorization: "Bearer $INTENT_DETECTION_API_KEY"
  Content-Type: "application/json"
requiredParameters:
  messages:
    description: "A list of messages comprising the conversation so far."
    options: []
  model:
    description: "ID of the model to use."
    options: ["gpt-4o", "gpt-4o-mini"]
optionalParameters:
  store:
    default: false
    description: "Whether or not to store the output of this chat completion request."
    options: [true, false]
  reasoning_effort:
    default: "medium"
    description: "Constrains effort on reasoning for reasoning models."
    options: ["low", "medium", "high"]
  metadata:
    description: "Developer-defined tags and values used for filtering completions in the dashboard."
    options: []
  frequency_penalty:
    default: 0
    description: "Penalize new tokens based on their existing frequency in the text so far."
    options: [-2.0, -1.0, 0, 1.0, 2.0]
  logit_bias:
    description: "Modify the likelihood of specified tokens appearing in the completion."
    options: []
  logprobs:
    default: false
    description: "Whether to return log probabilities of the output tokens or not."
    options: [true, false]
  top_logprobs:
    description: "Number of most likely tokens to return at each token position."
    options: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
  max_completion_tokens:
    description: "An upper bound for the number of tokens that can be generated for a completion."
    options: []
  n:
    default: 1
    description: "How many chat completion choices to generate for each input message."
    options: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  modalities:
    default: ["text"]
    description: "Output types that you would like the model to generate for this request."
    options: [["text"], ["text", "audio"]]
  prediction:
    description: "Configuration for a Predicted Output."
    options: []
  audio:
    description: "Parameters for audio output."
    options: []
  presence_penalty:
    default: 0
    description: "Penalize new tokens based on whether they appear in the text so far."
    options: [-2.0, -1.0, 0, 1.0, 2.0]
  response_format:
    description: "An object specifying the format that the model must output."
    options: []
  seed:
    description: "If specified, our system will make a best effort to sample deterministically."
    options: []
  service_tier:
    default: "auto"
    description: "Specifies the latency tier to use for processing the request."
    options: ["auto", "default"]
  stop:
    description: "Up to 4 sequences where the API will stop generating further tokens."
    options: []
  stream:
    default: false
    description: "If set, partial message deltas will be sent."
    options: [true, false]
  stream_options:
    description: "Options for streaming response."
    options: []
  temperature:
    default: 1
    description: "What sampling temperature to use."
    options: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2]
  top_p:
    default: 1
    description: "An alternative to sampling with temperature, called nucleus sampling."
    options: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
  tools:
    description: "A list of tools the model may call."
    options: []
  tool_choice:
    description: "Controls which (if any) tool is called by the model."
    options: ["none", "auto", {"type": "function", "function": {"name": "my_function"}}]
  parallel_tool_calls:
    default: true
    description: "Whether to enable parallel function calling during tool use."
    options: [true, false]
  user:
    description: "A unique identifier representing your end-user."
    options: []